{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Text Mining Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The goal of this project is to generate new text from various Shakespeare plays. We use Recurrent Neural Networks (RNNs) to do so. To assess the quality of our model, we compare the generated text to Shakespeare's originals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# All imports\n",
    "import tensorflow as tf\n",
    "import os.path\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data comes from Project Gutenberg. We downloaded the following books manually from the [Gutenberg website](https://www.gutenberg.org/ebooks/search/?query=shakespeare):\n",
    "\n",
    "Title | ID\n",
    "------|----\n",
    "Romeo and Juliet | 1112\n",
    "Hamlet | 1524\n",
    "Macbeth | 2264\n",
    "Midsummer Night's Dream| 2242\n",
    "The Tempest | 23042\n",
    "Othello | 2267\n",
    "The Tragedy Of Julius Caesar | 1120\n",
    "The Tragedy Of King Lear | 1128\n",
    "Twelth Night; Or, What You Will | 1526\n",
    "As You Like It | 1121\n",
    "The Taming Of The Shrew | 1107\n",
    "Henry V | 2253\n",
    "King Richard III | 1103\n",
    "\n",
    "The raw text files are stored in the subfolder `./data` of this project. We first write a helper class to read a single text, encode the text as an integer array and translate an integer array back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class shakespeare_text():\n",
    "    \"\"\"\n",
    "    Contains all data and methods for a given Shakespeare text.\n",
    "    \"\"\"\n",
    "    def __init__(self, id):\n",
    "        #Contains entire text of book\n",
    "        self.text = self.__read_text_(id)\n",
    "        \n",
    "        self.text_length = len(self.text)\n",
    "        #Characters in book\n",
    "        self.vocab = sorted(set(self.text))\n",
    "        #Dictionary of character to index mapping\n",
    "        self.vocab_to_int = {c: i for i, c in enumerate(self.vocab)}\n",
    "        #Total characters in book\n",
    "        self.vocab_length = len(self.vocab)\n",
    "        #Dictionary of index to character mapping\n",
    "        self.int_to_vocab = dict(enumerate(self.vocab))\n",
    "        #Characters in text mapped to indexes as numpy array\n",
    "        self.array = np.array([self.vocab_to_int[c] for c in self.text], dtype=np.int32)\n",
    "        #store batch size to object instance\n",
    "        self.batch_size = 40\n",
    "        self.step_size = 3\n",
    "\n",
    "\n",
    "    def __read_text_(self, id):\n",
    "        \"\"\"\n",
    "        Reads text corresponding to one of the ids.\n",
    "\n",
    "        :param id: string\n",
    "        \"\"\"\n",
    "        text_path = os.path.join('data', 'pg' + id + '.txt')\n",
    "        with open(text_path) as file:\n",
    "            txt = file.read()\n",
    "            \n",
    "        #Contains starting index of text for each book\n",
    "        #Used to remove copyright information from the text\n",
    "        pos_dict = {  '1112':[1799,159841],\n",
    "                      '1524':[11818,192814],\n",
    "                      '2264':[16241,119813],\n",
    "                      '2242':[15880,112106],\n",
    "                     '23042':[2230,142976],\n",
    "                      '2267':[15926,171448],\n",
    "                      '1120':[9030,140961],\n",
    "                      '1128':[9033,185348],\n",
    "                      '1526':[11920,131682],\n",
    "                      '1121':[2360,142514],\n",
    "                      '1107':[9030,148671],\n",
    "                      '2253':[11860,170304],\n",
    "                      '1103':[1950,200275] }\n",
    "        \n",
    "        return txt[pos_dict[id][0]:pos_dict[id][1]]\n",
    "    \n",
    "    def batches_gen(self, batch_size, seq_size):\n",
    "        \"\"\"\n",
    "        Batch generator that returns batches of size\n",
    "        batch_size x seq_size from array.\n",
    "\n",
    "        :param batch_size: Batch size, the number of sequences per batch\n",
    "        :seq_size: Number of characters per sequence.\n",
    "        \"\"\"\n",
    "        # Get the number of characters per batch and number of batches we can make\n",
    "        characters_per_batch = batch_size * seq_size\n",
    "        n_batches = len(self.array) // characters_per_batch\n",
    "\n",
    "        # Keep only enough characters to make full batches\n",
    "        arr = self.array[:characters_per_batch * n_batches]\n",
    "\n",
    "        # Reshape into n_seqs rows\n",
    "        arr = arr.reshape((batch_size, -1))\n",
    "\n",
    "        for n in range(0, arr.shape[1], seq_size):\n",
    "            # The features\n",
    "            x = arr[:, n:(n+characters_per_batch)]\n",
    "            # The targets, shifted by one\n",
    "            y = np.zeros_like(x)\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "            yield x, y\n",
    "            \n",
    "    def int_to_text(self, array):\n",
    "        \"\"\"\n",
    "        Converts an integer array to the corresponding text.\n",
    "        \n",
    "        :param array: numpy array of integers with values in the vocabulary\n",
    "        \"\"\"\n",
    "        int_list = array.tolist()\n",
    "        char_list = map(lambda x: self.int_to_vocab[x], int_list)\n",
    "        text = reduce(lambda x, y: x + y, char_list)\n",
    "        return text\n",
    "    \n",
    "    def print(self):\n",
    "        print(self.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the class written above to read in the Shakespeare plays. Then, for each play we train a separate RNN and use it to generate new text.\n",
    "\n",
    "For reusability, the script is split into a train and generate function. We will save the model from the train function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(id,obj,iterations=80):\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, obj.text_length - obj.batch_size, obj.step_size):\n",
    "        sentences.append(obj.text[i: i + obj.batch_size])\n",
    "        next_chars.append(obj.text[i + obj.batch_size])\n",
    "\n",
    "    #One hot encode \n",
    "    x = np.zeros((len(sentences),obj.batch_size, obj.vocab_length), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), obj.vocab_length), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, obj.vocab_to_int[char]] = 1\n",
    "            y[i, obj.vocab_to_int[next_chars[i]]] = 1\n",
    "    \n",
    "    #Define the LSTM model\n",
    "    #Long-Short Term Memory layer - Hochreiter 1997 with tanh activation function\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(obj.batch_size, obj.vocab_length)))\n",
    "    model.add(Dense(obj.vocab_length))\n",
    "    model.add(Activation('softmax'))\n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    # train the model\n",
    "    for iteration in range(1, iterations):\n",
    "        print()\n",
    "        print('-' * 50)\n",
    "        print('Iteration', iteration)\n",
    "        model.fit(x, y,\n",
    "                  batch_size=128,\n",
    "                  epochs=1)\n",
    "    \n",
    "    #Save model for future use\n",
    "    model.save('model_' + id + '.h5') \n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generate function starts at a random position in the text and uses it as a \"seed\". We then create 4 different sequences from different multinomial sampling distributions. Each generated sequence is 2000 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate(id,obj,model):\n",
    "\n",
    "    def sample(preds, temperature=1.0):\n",
    "        # helper function to sample an index from a probability array\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "        return np.argmax(probas)\n",
    "    \n",
    "    start_index = random.randint(0, obj.text_length - obj.batch_size - 1)\n",
    "    gen_text = ''\n",
    "    \n",
    "    # generate 4 sequences of 2000 characters \n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        generated = ''\n",
    "        sentence = obj.text[start_index: start_index + obj.batch_size]\n",
    "        generated += sentence\n",
    "              \n",
    "        for i in range(2000):\n",
    "            x_pred = np.zeros((1, obj.batch_size, obj.vocab_length))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, obj.vocab_to_int[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = obj.int_to_vocab[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "            \n",
    "        gen_text+=('\\n'+generated)\n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will write the generated text to an external file for testing. The files we generated were trained for 80 epochs till we obtained logloss of around .6. We found that around 40 epochs we started seeing identifiable word structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/1\n",
      "52668/52668 [==============================] - 57s 1ms/step - loss: 2.3035\n"
     ]
    }
   ],
   "source": [
    "def write_shakespeare(id):\n",
    "    #instantiate object to pass to subsequent functions\n",
    "    shakespeare = shakespeare_text(id)\n",
    "    \n",
    "    #Setting to only one iteration for the final compilation\n",
    "    #trm contains the trained model. To perform in batch use load_model\n",
    "    trm = train(id,shakespeare,iterations=2)\n",
    "    \n",
    "    #Write generated text to external file\n",
    "    print(generate(id,shakespeare,trm), file=open(\"./gen_data/gen_\"+ id +\"x.txt\", \"w\"))\n",
    "\n",
    "\n",
    "\n",
    "write_shakespeare('1112')\n",
    "#write_shakespeare('1524')\n",
    "#write_shakespeare('2264')\n",
    "#write_shakespeare('2242')\n",
    "#write_shakespeare('23042')\n",
    "#write_shakespeare('2267')\n",
    "#write_shakespeare('1120')\n",
    "#write_shakespeare('1128')\n",
    "#write_shakespeare('1526')\n",
    "#write_shakespeare('1121')\n",
    "#write_shakespeare('1107')\n",
    "#write_shakespeare('2253')\n",
    "#write_shakespeare('1103')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Here is a short excerpt of generated from Julius Ceasar:\n",
    "\n",
    "\"Exit Brutus.\n",
    "    I dvey, ware you all your hands and stinn.\n",
    "  CASSIUS. Most noble be some stay athy leess.\n",
    "    I had rey bears at a seeator and foan\n",
    "    To what thou somens.\n",
    "  BRUTUS. I comes to but our hands and that I do not day\n",
    "    Oven men are and yet he was ambitious, Brutus.\n",
    "    Stale you is come to you, if words if Malker Cassius\n",
    "worse.\"\n",
    "\n",
    "We can see that the LSTM has managed to identify characters and that their dialogue should begin with capatilized names. \n",
    "Basic sentences structure has also been learnt. Sentences start with capital letters and end with full stops.\n",
    "We can also, however, see that Brutus has exited before delivering his line, so there is still some contextual cues needing to be learned.\n",
    "Brutus also appears to be talking to himself in this scene.\n",
    "\n",
    "So, although far from perfect, our generator does generate somewhat passable Shakespeare to the human eye, provided that you dont look to deep.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an RNN to discriminate between the Shakespeare plays. The goal is to have a classification model based on sequences of text with the target variables corresponding to the different plays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of generated text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with perplexity score\n",
    "\n",
    "We first evaluate our generated model using the **perplexity score**. For that we first have to evaluate the perplexity score of each of our models on the original dataset, i.e. we produce a matrix where the rows $i$ correspond to the model trained on play $i$, the columns $j$ correspond to the text of play $j$ and the entries correpond to the perplexity score of model $i$ evaluated on text $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we construct the same matrix, but this time we evaluate the models not on the original plays, but on the generated text. This gives another matrix of perplexity scoers. If our generated model worked well, then the two matrices should look relatively similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with discriminator\n",
    "\n",
    "Next, we can use the trained discriminator to evaluate the generated text. If our text generation model is good, it should achieve accuracy similar to the accuracy in the original plays. It should not work better or worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also show the confusion matrix. This should also be similar to the one from the original plays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
